{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Umerfarooq122/Text-Summarization-by-Fine-tuning-a-pre-trained-transformer-PEGASUS-from-hugging-face-library/blob/main/PEGASUS_on_Samsum%2C_PubMed_and_Arxiv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOoAGzuEcKXI"
      },
      "source": [
        "### **Text Summarization Using Pre-training with Extracted Gap-sentences for Abstractive Summarization (PEGASUS) from Huggingface**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRpVKqRocxza"
      },
      "source": [
        "#### **Environment Setup:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13XOm5WdnteE"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece] datasets sacrebleu rouge_metric py7zr -q\n",
        "!pip3 install -q -U bitsandbytes\n",
        "!pip3 install -q -U peft\n",
        "!pip3 install -q -U trl\n",
        "!pip3 install -q -U accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mKjCs7VWieW",
        "outputId": "112135f8-c082-417b-e0e8-139142b7dcbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.6)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets --upgrade\n",
        "!pip install evaluate --upgrade\n",
        "!pip install rouge_score --upgrade\n",
        "!pip install transformers --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmesKwR6HbDW",
        "outputId": "7c277bab-bebc-42b4-f4aa-b4102ba8eff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig, BigBirdPegasusForConditionalGeneration, PegasusForConditionalGeneration, PegasusTokenizer\n",
        "from peft import PeftModel, LoraConfig\n",
        "from datasets import load_dataset\n",
        "from evaluate import load\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Acc0g0g8IPCK",
        "outputId": "652d4231-e43a-4bf3-ce71-4c62d1385393"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4T3VmgDdAVt"
      },
      "source": [
        "### **PEGASUS large:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAoHkcexO3Kj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72b0cf1f-3249-46ea-d498-52551de14ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_large = \"google/pegasus-large\"\n",
        "tokenizer_large = AutoTokenizer.from_pretrained(\"google/pegasus-large\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Set the environment variable to enable more verbose error messages\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "metadata": {
        "id": "GD3Eu4MQgeyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KW3Hzl6Pbig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df8cdf24-d5cc-49b1-c9ca-86081a182a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_pega = AutoModelForSeq2SeqLM.from_pretrained(model_large)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqE-Z-8TdHi1"
      },
      "source": [
        "#### **PubMED Dataset:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fACpPUNFQI0U",
        "outputId": "f9c539e1-8198-4255-b327-519cf01fcdfc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 14732\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 819\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 818\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "dataset_science = load_dataset(\"ccdv/pubmed-summarization\", trust_remote_code=True)\n",
        "dataset_science"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGPQ0VLwdO-F"
      },
      "source": [
        "##### **Splitting the Dataset:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mQvVM1-QWR7",
        "outputId": "6e7c467b-2125-490a-8c5a-796e84a25118"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[14732, 819, 818]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "split_lengths = [len(dataset_science[split]) for split in dataset_science]\n",
        "split_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM6GuZheQwCE",
        "outputId": "445ef94d-ead1-4e5e-fdc8-9c82751146a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: ['id', 'dialogue', 'summary']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Features: {dataset_science['train'].column_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta_J5F7kQzKm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9408a5fe-1862-413b-bfdc-c4f47101d4bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Article:\n",
            "Amanda: I baked  cookies. Do you want some?\r\n",
            "Jerry: Sure!\r\n",
            "Amanda: I'll bring you tomorrow :-)\n",
            "\n",
            "Abstract:\n",
            "Amanda baked cookies and will bring Jerry some tomorrow.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nArticle:\")\n",
        "print(dataset_science[\"train\"][0]['article'])\n",
        "print(\"\\nAbstract:\")\n",
        "print(dataset_science[\"train\"][0]['abstract'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43IjDWVzdmO3"
      },
      "source": [
        "##### **Test Running Raw Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZamWJiDRItO"
      },
      "outputs": [],
      "source": [
        "dialogue = dataset_science[\"train\"][1][\"article\"]\n",
        "summary = dataset_science[\"train\"][1][\"abstract\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2NSNHznS_FT"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\"summarization\", model=model_pega, tokenizer=tokenizer_large, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV3YrJfDwdxE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4VSnMMxTQ7B"
      },
      "outputs": [],
      "source": [
        "\n",
        "max_length = 4000 # Set a suitable maximum length for your input\n",
        "\n",
        " #Truncate dialogue if it exceeds max_length\n",
        "if len(dialogue) > max_length:\n",
        "    dialogue = dialogue[:max_length]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bV4SK7C8mL2w",
        "outputId": "2cfd9d5b-337d-450f-894e-66071d521b31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Olivia: Who are you voting for in this election? \\r\\nOliver: Liberals as always.\\r\\nOlivia: Me too!!\\r\\nOliver: Great'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "\n",
        "dialogue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "M7aCim-RYnZc",
        "outputId": "dc4250ad-f8b5-45b9-f171-d4455d5e4bc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Olivia and Olivier are voting for liberals in this election. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l31DX7afhL0k"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer_large, model = model_pega)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "S2vb1okGmIsx",
        "outputId": "9aa6299a-4f3c-48c5-9a2e-37d2a6912a1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 128, but your input_length is only 26. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Olivia: Who's you voting for in this election? Oliver: Liberals as always .<n>Olivia: Me too!!<n>Oliver: Great .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "pipe_out = pipe(dialogue)\n",
        "pipe_outp = pipe_out[0][\"summary_text\"]\n",
        "pipe_outp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDYGq1bRymvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ee652e-51fb-4ffc-9696-b32a71af2b9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': ''}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "pipe_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO_OPHI9TwpE"
      },
      "outputs": [],
      "source": [
        "#print(pipe_outp.replace(\" . <n>\", \".\\n\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GSqmM9OgNg0"
      },
      "outputs": [],
      "source": [
        "#summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4TQW8Fhdw5t"
      },
      "source": [
        "##### **Evaluating The Performance of Raw Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V17Hqfy_Ud26"
      },
      "outputs": [],
      "source": [
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
        "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
        "    for i in range(0, len(list_of_elements), batch_size):\n",
        "        yield list_of_elements[i : i + batch_size]\n",
        "\n",
        "def calculate_metric_on_TEST_ds(dataset, metric, model, tokenizer, batch_size=16, device=device, column_text=\"article\", column_summary=\"abstract\"):\n",
        "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
        "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
        "    for article_batch, target_batch in tqdm(\n",
        "        zip(article_batches, target_batches), total=len(article_batches)):\n",
        "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
        "                        padding=\"max_length\", return_tensors=\"pt\")\n",
        "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
        "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "                         length_penalty=0.8, num_beams=8, max_length=128)\n",
        "\n",
        "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
        "                                clean_up_tokenization_spaces=True)\n",
        "               for s in summaries]\n",
        "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
        "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "    return metric.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvb0NdAFUI-S",
        "outputId": "9516e8fb-97d3-4b30-deec-a99c108744c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:09<00:00,  1.87s/it]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "rouge_metric = load(\"rouge\")\n",
        "\n",
        "score = calculate_metric_on_TEST_ds(dataset_science[\"train\"][0:10], rouge_metric, model_pega, tokenizer_large, batch_size=2, column_text=\"article\", column_summary='abstract')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AICK8NqEaRI7"
      },
      "outputs": [],
      "source": [
        "\n",
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "\n",
        "rouge_dict = {rn: score[rn] for rn in rouge_names}\n",
        "print(pd.DataFrame(rouge_dict, index=[\"pegasus\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrAOQFzwd7cb"
      },
      "source": [
        "##### **Fine-Tuning PEGASUS:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH3Y1BVieNau"
      },
      "source": [
        "###### **Tokenizing The Text:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwXW-OWMynxq",
        "outputId": "32415a34-de28-4d30-974a-0918143cfeff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4845\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer_large(dialogue, return_tensors=\"pt\").input_ids\n",
        "num_tokens = tokens.shape[1]\n",
        "print(num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53t_7SuP4MWg"
      },
      "outputs": [],
      "source": [
        "def convert_examples_to_features(example_batch):\n",
        "\n",
        "  input_encodings = tokenizer_large(example_batch[\"dialogue\"], max_length=1024, truncation=True)\n",
        "  with tokenizer_large.as_target_tokenizer():\n",
        "      target_encodings = tokenizer_large(example_batch[\"summary\"], max_length=256, truncation=True)\n",
        "  return {\n",
        "        \"input_ids\": input_encodings[\"input_ids\"],\n",
        "        \"attention_mask\": input_encodings[\"attention_mask\"],\n",
        "        \"labels\": target_encodings[\"input_ids\"]\n",
        "    }\n",
        "dataset_science_pt = dataset_science.map(convert_examples_to_features, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eojyuIvJ58kL"
      },
      "outputs": [],
      "source": [
        "#dataset_science_pt[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT_SDMyO6FI4"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer_large, model = model_pega)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(r = 8, target_modules = [\"q_proj\",\"o_proj\",\"k_proj\", \"v_proj\",\n",
        "                                                  \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                         task_type = \"CAUSAL_LM\",)"
      ],
      "metadata": {
        "id": "JgNnu8qqxlhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbIweZnweFBq"
      },
      "source": [
        "###### **Setting The Training Arguments:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "trainer_args = TrainingArguments(\n",
        "    output_dir=\"pegasus-science\",\n",
        "    num_train_epochs=5,  # Reduced for monitoring potential overfitting\n",
        "    warmup_steps=500,\n",
        "    per_device_train_batch_size=2,  # Increased batch size, if possible\n",
        "    per_device_eval_batch_size=2,\n",
        "    weight_decay=0.005,  # Adjusted for potential improvement\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=20,  # More frequent evaluations\n",
        "    save_steps=1000,  # More frequent saves\n",
        "    gradient_accumulation_steps=8,  # Reduced to speed up updates\n",
        "    fp16=True  # Enable mixed precision if hardware supports it\n",
        ")\n"
      ],
      "metadata": {
        "id": "-w4fg68MX5gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnXo4f8HeY9t"
      },
      "source": [
        "###### **Training The Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0ww1EjU7Aoe"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(model=model_pega, args=trainer_args,\n",
        "                  processing_class=tokenizer_large, data_collator=seq2seq_data_collator,\n",
        "                  train_dataset=dataset_science_pt[\"train\"],\n",
        "                  eval_dataset=dataset_science_pt[\"validation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh9pvzDa0-5M"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kW36H6_egqb"
      },
      "source": [
        "###### **Evaluating The Performance:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4OvmCf15Fnx"
      },
      "outputs": [],
      "source": [
        "score = calculate_metric_on_TEST_ds(dataset_science[\"test\"], rouge_metric, model_pega, tokenizer_large, batch_size=2, column_text=\"article\", column_summary=\"abstract\")\n",
        "rouge_dict = {rn: score[rn] for rn in rouge_names}\n",
        "pd.DataFrame(rouge_dict, index=[\"pegasus-PubMed\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyr3Ysz9en6Z"
      },
      "source": [
        "###### **Saving The Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNYkw_YjmzzX"
      },
      "outputs": [],
      "source": [
        "model_pega.save_pretrained(\"pegasus-science-model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SOptqqAs6Y-"
      },
      "outputs": [],
      "source": [
        "tokenizer_large.save_pretrained(\"pegasus-science-tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI0jqvK48klk"
      },
      "source": [
        "#### **Arxiv Dataset:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Loading Dataset:**"
      ],
      "metadata": {
        "id": "2GrdIJCMDNMu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_-ya8zP8pUz"
      },
      "outputs": [],
      "source": [
        "data_arxiv = load_dataset(\"ccdv/arxiv-summarization\", trust_remote_code=True)\n",
        "data_arxiv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Tokenizing Data:**"
      ],
      "metadata": {
        "id": "tJ0tXlOxDlna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0LJunPBRJIo"
      },
      "outputs": [],
      "source": [
        "def convert_examples_to_features(example_batch):\n",
        "\n",
        "  input_encodings = tokenizer_large(example_batch[\"article\"], max_length=1024, truncation=True)\n",
        "  with tokenizer_large.as_target_tokenizer():\n",
        "      target_encodings = tokenizer_large(example_batch[\"abstract\"], max_length=256, truncation=True)\n",
        "  return {\n",
        "        \"input_ids\": input_encodings[\"input_ids\"],\n",
        "        \"attention_mask\": input_encodings[\"attention_mask\"],\n",
        "        \"labels\": target_encodings[\"input_ids\"]\n",
        "    }\n",
        "dataset_arxiv_pt = data_arxiv.map(convert_examples_to_features, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Setting the training parameters:**"
      ],
      "metadata": {
        "id": "JKIIdRyBEXiv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIJ2qZUW_aAQ"
      },
      "outputs": [],
      "source": [
        "trainer_args = TrainingArguments(\n",
        "    output_dir=\"pegasus-science\",\n",
        "    num_train_epochs=5,  # Reduced for monitoring potential overfitting\n",
        "    warmup_steps=500,\n",
        "    per_device_train_batch_size=2,  # Increased batch size, if possible\n",
        "    per_device_eval_batch_size=2,\n",
        "    weight_decay=0.005,  # Adjusted for potential improvement\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=20,  # More frequent evaluations\n",
        "    save_steps=1000,  # More frequent saves\n",
        "    gradient_accumulation_steps=8,  # Reduced to speed up updates\n",
        "    fp16=True  # Enable mixed precision if hardware supports it\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Training The Model:**"
      ],
      "metadata": {
        "id": "QBW1hwWyEqar"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcqK_QmXRAru"
      },
      "outputs": [],
      "source": [
        "Arxtrainer = SFTTrainer(model=model_pega, args=trainer_args,\n",
        "                  processing_class=tokenizer_large, data_collator=seq2seq_data_collator,\n",
        "                  train_dataset=dataset_arxiv_pt[\"train\"],\n",
        "                  eval_dataset=dataset_arxiv_pt[\"validation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u99ZmNc6DAwH"
      },
      "outputs": [],
      "source": [
        "Arxtrainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Evaluating The Performance:**"
      ],
      "metadata": {
        "id": "jh9VndQ1FJ7W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--kl9oMv3Vma"
      },
      "outputs": [],
      "source": [
        "score = calculate_metric_on_TEST_ds(data_arxiv[\"test\"], rouge_metric, model_pega, tokenizer_large, batch_size=2, column_text=\"article\", column_summary=\"abstract\")\n",
        "rouge_dict = {rn: score[rn] for rn in rouge_names}\n",
        "pd.DataFrame(rouge_dict, index=[\"pegasus-Arxiv\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Samsum Dataset:**"
      ],
      "metadata": {
        "id": "AIA67YW5GXn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Loading Dataset:**"
      ],
      "metadata": {
        "id": "hFs07BVPGgbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_samsum = load_dataset(\"Samsung/samsum\", trust_remote_code=True)\n",
        "data_samsum"
      ],
      "metadata": {
        "id": "Wd923CpNGfvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Tokenizing Data:**"
      ],
      "metadata": {
        "id": "Hn93sseIGm-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(example_batch):\n",
        "\n",
        "  input_encodings = tokenizer_large(example_batch[\"dialogue\"], max_length=1024, truncation=True)\n",
        "  with tokenizer_large.as_target_tokenizer():\n",
        "      target_encodings = tokenizer_large(example_batch[\"summary\"], max_length=256, truncation=True)\n",
        "  return {\n",
        "        \"input_ids\": input_encodings[\"input_ids\"],\n",
        "        \"attention_mask\": input_encodings[\"attention_mask\"],\n",
        "        \"labels\": target_encodings[\"input_ids\"]\n",
        "    }\n",
        "dataset_samsum_pt = data_samsum.map(convert_examples_to_features, batched=True)"
      ],
      "metadata": {
        "id": "FHhuvaabGqBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Training The Model:**"
      ],
      "metadata": {
        "id": "2hSd099SHHDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samtrainer = SFTTrainer(model=model_pega, args=trainer_args,\n",
        "                  processing_class=tokenizer_large, data_collator=seq2seq_data_collator,\n",
        "                  train_dataset=dataset_samsum_pt[\"train\"],\n",
        "                  eval_dataset=dataset_samsum_pt[\"validation\"])"
      ],
      "metadata": {
        "id": "z3UwbnJsHHLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samtrainer.train()"
      ],
      "metadata": {
        "id": "qd5PVn2THQSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Evaluating The Performance:**"
      ],
      "metadata": {
        "id": "SkHtn7q0HScJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = calculate_metric_on_TEST_ds(data_samsum[\"test\"], rouge_metric, model_pega, tokenizer_large, batch_size=2, column_text=\"dialogue\", column_summary=\"summary\")\n",
        "rouge_dict = {rn: score[rn] for rn in rouge_names}\n",
        "pd.DataFrame(rouge_dict, index=[\"pegasus-Samsum\"])"
      ],
      "metadata": {
        "id": "_0YZoendHVbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PEGASUS BigBird:**"
      ],
      "metadata": {
        "id": "UsQyoPLJHkcO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **PubMed:**"
      ],
      "metadata": {
        "id": "typ4z-d1ku-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Loading The Model and Tokenizer:**"
      ],
      "metadata": {
        "id": "aOFViJSDH2ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_big = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-pubmed\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "model_bigbird = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-pubmed\")\n",
        "#model_pega = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")"
      ],
      "metadata": {
        "id": "yitAZQfAH-FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Tokenizing The Data:**"
      ],
      "metadata": {
        "id": "8OfFjNNEf8O4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(example_batch):\n",
        "\n",
        "  input_encodings = tokenizer_big(example_batch[\"article\"], max_length=1024, truncation=True)\n",
        "  with tokenizer_large.as_target_tokenizer():\n",
        "      target_encodings = tokenizer_big(example_batch[\"abstract\"], max_length=256, truncation=True)\n",
        "  return {\n",
        "        \"input_ids\": input_encodings[\"input_ids\"],\n",
        "        \"attention_mask\": input_encodings[\"attention_mask\"],\n",
        "        \"labels\": target_encodings[\"input_ids\"]\n",
        "    }\n",
        "dataset_science_pt_big = dataset_science.map(convert_examples_to_features, batched=True)"
      ],
      "metadata": {
        "id": "Bwd8_wuhf8Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Model Parameters:**"
      ],
      "metadata": {
        "id": "oK6Bo7K2g_Zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_args = TrainingArguments(\n",
        "    output_dir=\"pegasus-science\",\n",
        "    num_train_epochs=5,  # Reduced for monitoring potential overfitting\n",
        "    warmup_steps=500,\n",
        "    per_device_train_batch_size=2,  # Increased batch size, if possible\n",
        "    per_device_eval_batch_size=2,\n",
        "    weight_decay=0.005,  # Adjusted for potential improvement\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=20,  # More frequent evaluations\n",
        "    save_steps=1000,  # More frequent saves\n",
        "    gradient_accumulation_steps=8,  # Reduced to speed up updates\n",
        "    fp16=True  # Enable mixed precision if hardware supports it\n",
        ")"
      ],
      "metadata": {
        "id": "tWv0kgSNhGz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Training Model:**"
      ],
      "metadata": {
        "id": "ZbXttV6LhM7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigptrainer = SFTTrainer(model=model_bigbird, args=trainer_args,\n",
        "                  processing_class=tokenizer_big, data_collator=seq2seq_data_collator,\n",
        "                  train_dataset=dataset_science_pt_big[\"train\"],\n",
        "                  eval_dataset=dataset_science_pt_big[\"validation\"])"
      ],
      "metadata": {
        "id": "xLcioNEahMPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigptrainer.train()"
      ],
      "metadata": {
        "id": "ySfJZsZ9hdQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Evaluating The Model's Performance:**"
      ],
      "metadata": {
        "id": "e7fCnTIGkBYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = calculate_metric_on_TEST_ds(dataset_science[\"test\"], rouge_metric, model_bigbird, tokenizer_big, batch_size=2, column_text=\"article\", column_summary=\"abstract\")\n",
        "rouge_dict = {rn: score[rn] for rn in rouge_names}\n",
        "pd.DataFrame(rouge_dict, index=[\"pegasus-big-PubMed\"])"
      ],
      "metadata": {
        "id": "6Op181mNkAjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Arxiv:**"
      ],
      "metadata": {
        "id": "fVKhn-3NmA2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Tokenizing The Data:**"
      ],
      "metadata": {
        "id": "PrWI6hHSmFhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(example_batch):\n",
        "\n",
        "  input_encodings = tokenizer_big(example_batch[\"article\"], max_length=1024, truncation=True)\n",
        "  with tokenizer_large.as_target_tokenizer():\n",
        "      target_encodings = tokenizer_big(example_batch[\"abstract\"], max_length=256, truncation=True)\n",
        "  return {\n",
        "        \"input_ids\": input_encodings[\"input_ids\"],\n",
        "        \"attention_mask\": input_encodings[\"attention_mask\"],\n",
        "        \"labels\": target_encodings[\"input_ids\"]\n",
        "    }\n",
        "dataset_arxiv_pt_big = data_arxiv.map(convert_examples_to_features, batched=True)"
      ],
      "metadata": {
        "id": "3HkML83xmIDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Training Model:**"
      ],
      "metadata": {
        "id": "mOHPaVc_makK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigatrainer = SFTTrainer(model=model_bigbird, args=trainer_args,\n",
        "                  processing_class=tokenizer_big, data_collator=seq2seq_data_collator,\n",
        "                  train_dataset=dataset_arxiv_pt_big[\"train\"],\n",
        "                  eval_dataset=dataset_arxiv_pt_big[\"validation\"])"
      ],
      "metadata": {
        "id": "wgmJkZcCmbay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigatrainer.train()"
      ],
      "metadata": {
        "id": "nPWHU0yxmmRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Evaluating The Model's Performance:**"
      ],
      "metadata": {
        "id": "Q3nEnvg3mn36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = calculate_metric_on_TEST_ds(data_arxiv[\"test\"], rouge_metric, model_bigbird, tokenizer_big, batch_size=2, column_text=\"article\", column_summary=\"abstract\")\n",
        "rouge_dict = {rn: score[rn] for rn in rouge_names}\n",
        "pd.DataFrame(rouge_dict, index=[\"pegasus-big-arxiv\"])"
      ],
      "metadata": {
        "id": "nmkiC5K3mq_g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1gI5NqBtSVwi-cQC1geuoYa24MkI_26Ts",
      "authorship_tag": "ABX9TyOWJAYcdcg9pS+5NEfDp3Ty",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}